{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e050036a-9701-47c0-b0de-2251c53e5591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import maketab as mt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "plt.rcParams['mathtext.fontset'] = 'cm'\n",
    "import gc\n",
    "import time\n",
    "\n",
    "\n",
    "sec_norm = 410\n",
    "cutoff = 50 #cutting of time it takes to lift off\n",
    "train_dir = (['data/24-1-25/', 'data/4-2-25/', 'data/5-2-25/'])\n",
    "test_dir = (['data/31-1-25/', 'data/11-4-25/', 'data/9-4-25/'])\n",
    "sig_norm = [3.7, 2.3]\n",
    "PATH = 'bat_pics/bat/lnu/'\n",
    "\n",
    "def norm(signal, norm):\n",
    "    signal = (signal - norm[1]) / (norm[0] - norm[1])\n",
    "    return signal\n",
    "\n",
    "def load_data(path_dir):\n",
    "    t, signal = mt.battery(path_dir)\n",
    "    secleft = t[-1]/1000\n",
    "    tleft = 1 - t / max(t)\n",
    "    tleft = tleft*(secleft/sec_norm)\n",
    "    signal = norm(signal[cutoff:], sig_norm)\n",
    "    tleft = tleft[cutoff:]\n",
    "    t = t[cutoff:]\n",
    "    normalized_train = np.array([signal, tleft])\n",
    "    return normalized_train\n",
    "\n",
    "def make_data(train_dir):\n",
    "    if(len(train_dir) == 1):\n",
    "        for d in train_dir:\n",
    "            train_data = load_data(d)\n",
    "        #plt.plot(train_data[0])\n",
    "        #plt.plot(train_data[1])\n",
    "        #plt.show()\n",
    "        return train_data\n",
    "    else:\n",
    "        #train_data = np.empty((len(train_dir), 0))\n",
    "        train_data = load_data(train_dir[0])\n",
    "        for d in train_dir[1:]:\n",
    "            single_data = load_data(d)\n",
    "            train_data = np.concatenate((train_data, single_data), axis=1)\n",
    "        #plt.plot(train_data[0])\n",
    "        #plt.plot(train_data[1])\n",
    "        #plt.plot(train_data[2])\n",
    "        #plt.show()\n",
    "        return train_data\n",
    "\n",
    "class FunctionDataset(Dataset):\n",
    "    def __init__(self, data, n):\n",
    "        self.g = data[0]  # Function g\n",
    "        self.f = data[1]  # Function f\n",
    "        self.n = n  # Sequence length\n",
    "        self.total_length = len(self.g)\n",
    "\n",
    "    def __len__(self):\n",
    "        # The number of sequences is the total length minus the sequence length plus 1\n",
    "        return self.total_length - self.n + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get a sequence of g and the corresponding sequence of f\n",
    "        g_seq = self.g[idx:idx + self.n]\n",
    "        f_seq = self.f[idx:idx + self.n]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        g_tensor = torch.tensor(g_seq, dtype=torch.float32)\n",
    "        f_tensor = torch.tensor(f_seq, dtype=torch.float32)\n",
    "\n",
    "        return g_tensor, f_tensor\n",
    "\n",
    "def create_dataloader(data, seq_length, batch_size=1, shuffle=False):\n",
    "    dataset = FunctionDataset(data, seq_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.register_buffer(\"hardcoded_matrix\", torch.ones(output_size, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        #out = self.sig(out)\n",
    "        out = out @ self.hardcoded_matrix.t()\n",
    "        return out\n",
    "\n",
    "def train(num_epochs, dataloader, model, criterion, optimizer):\n",
    "    epoch_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0  # To accumulate loss for the entire epoch\n",
    "        num_batches = 0\n",
    "        for batch_X, batch_Y in dataloader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_Y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        avg_loss = (epoch_loss / num_batches)\n",
    "        epoch_losses.append(avg_loss)\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.8f}')\n",
    "    # plt.plot(epoch_losses)\n",
    "    # plt.xlabel(\"Epoch\")\n",
    "    # plt.ylabel(\"Loss\")\n",
    "    # plt.show()\n",
    "\n",
    "def evaluate_and_plot(data, model, n, name):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    g = data[0]  # Input function g\n",
    "    f = data[1]  # True function f\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for i in range(0, len(g)-n, n):\n",
    "            input_seq = g[i:i+n]\n",
    "            # Convert to PyTorch tensor and add batch dimension\n",
    "            X = torch.tensor(input_seq, dtype=torch.float32).unsqueeze(0)\n",
    "            y = model(X)\n",
    "            predictions.extend(y.squeeze(0).numpy())  # Remove batch dimension and convert to numpy\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(g, label=r'$g_t$')\n",
    "    plt.plot(f, label=r'${f_t}$')\n",
    "    plt.plot(predictions, label=r'$y_t$')\n",
    "\n",
    "    mse = mean_squared_error(f[:len(predictions)], predictions)\n",
    "    r2 = r2_score(f[:len(predictions)], predictions)\n",
    "    text = (\n",
    "        f\"{'MSE':<5} {mse:>.4e}\\n\"\n",
    "        f\"{'R2':<5} {r2:>.4e}\"\n",
    "    )\n",
    "\n",
    "    # plt.text(\n",
    "    #     0.0, 0.0,\n",
    "    #     text,\n",
    "    #     fontsize=11,\n",
    "    #     fontfamily=\"monospace\",  # Use a monospaced font\n",
    "    #     verticalalignment=\"center\",  # Align text vertically\n",
    "    #     horizontalalignment=\"left\",  # Align text horizontally\n",
    "    #     bbox=dict(facecolor=\"lightgray\", alpha=0.8, edgecolor=\"black\"),  # Add a background box\n",
    "    # )\n",
    "\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('t(ms)')\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.grid(True)\n",
    "    #plt.show()\n",
    "    plt.savefig(PATH+name+'.pdf')\n",
    "    plt.close()\n",
    "    return mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2245e993-e743-41de-9812-6fa1e268af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(n, k, num_epochs, batch_size, learning_rate):\n",
    "    # # Number of values to predict at a time\n",
    "    # n = 30\n",
    "    # # Hidden size\n",
    "    # k = 50\n",
    "    # num_epochs = 10\n",
    "    # batch_size = 16\n",
    "    # learning_rate = 0.001\n",
    "\n",
    "    model = MLP(n, k, n)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_data = make_data(train_dir)\n",
    "    test_data = make_data(test_dir)\n",
    "    dataloader = create_dataloader(train_data, n, batch_size=batch_size, shuffle=True)\n",
    "    start_time = time.time()\n",
    "    #train(num_epochs, dataloader, model, criterion, optimizer)\n",
    "    end_time = time.time()\n",
    "    print(f\"Training completed in {(end_time - start_time):.4f} seconds\")\n",
    "    mse1, r21 = evaluate_and_plot(train_data, model, n, 'train'+str(n))\n",
    "    mse2, r22 = evaluate_and_plot(test_data, model, n, 'test'+str(n))\n",
    "\n",
    "    # Force cleanup (optional)\n",
    "    del model, criterion, optimizer\n",
    "    gc.collect()  # Garbage collect to ensure no lingering references\n",
    "\n",
    "    return n, learning_rate, mse1, mse2, r21, r22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90c024da-c449-433a-be1e-e3ff6b98cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tolat(df):\n",
    "    return \"\\n\".join(\" & \".join(f\"{{${val}$}}\" for val in row) + \" \\\\\\\\\" \n",
    "                    for _, row in df.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "badd6d1f-d662-4537-ad62-bfa8825fd1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in 0.0000 seconds\n",
      "    n      lr  train_mse  test_mse  train_r2  test_r2\n",
      "0  40  0.0001     1.0493     1.027    -12.45  -12.207\n",
      "{$40.0$} & {$0.0001$} & {$1.0493$} & {$1.027$} & {$-12.45$} & {$-12.207$} \\\\\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    rows = []\n",
    "    #columns = ['n', 'k', 'num_epochs', 'batch_size', 'lr', 'train_mse', 'train_r2', 'test_mse', 'test_r2']\n",
    "    columns = ['n', 'lr', 'train_mse', 'test_mse', 'train_r2', 'test_r2']\n",
    "    #rows.append(test_model(20, 1, 100, 16, 0.0001))\n",
    "    rows.append(test_model(40, 1, 100, 16, 0.0001))\n",
    "    #rows.append(test_model(60, 1, 100, 16, 0.0001))\n",
    "    #rows.append(test_model(80, 1, 100, 16, 0.0001))\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    df = df.round({\n",
    "    'train_mse': 4,\n",
    "    'test_mse': 4,\n",
    "    'train_r2': 3,\n",
    "    'test_r2': 3\n",
    "    })\n",
    "    print(df)\n",
    "    #print(test_model(30, 50, 10, 16, 0.001))\n",
    "    print(\"\\n\".join(\" & \".join(f\"{{${val}$}}\" for val in row) + \" \\\\\\\\\" for _, row in df.iterrows()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39035bd-4786-4fcf-b0a2-86f638e893ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
